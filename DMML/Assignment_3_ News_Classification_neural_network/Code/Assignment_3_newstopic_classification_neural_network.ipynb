{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sk-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score ,f1_score ,recall_score, precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "f = open(\"/home/arolive/Documents/CMI/MSC_2nd_sem/Machine_learning/My_work/Assignment_3/Data/News-Classification-DataSet.json\",\"r\")\n",
    "for line in f:\n",
    "     data_all.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each array of data is a dictionary of each news items consisting  'content', 'annotation', 'extras', 'metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'annotation', 'extras', 'metadata'])\n",
      "dict_keys(['notes', 'label'])\n"
     ]
    }
   ],
   "source": [
    "print(data_all[0].keys())\n",
    "print(data_all[0][\"annotation\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From each of the list we need contents and annotation ~> label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "label= []\n",
    "for i in data_all:\n",
    "    content.append(i[\"content\"])\n",
    "    label.append(i[\"annotation\"][\"label\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing noise from contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noice = [\"\\\\\",\"-\",\",\",\",\",\"\\n\",\"#\",\";\",\".\",\"'\",\"(\",\")\",\"@\",\"!\",\"$\",\"&\",\"%\"]\n",
    "for line in range(len(content)): \n",
    "    for i in noice:\n",
    "        content[line] = content[line].replace(str(i),\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"content\" : content,  \"label\" : label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing data and dropping duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7600</td>\n",
       "      <td>7600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sysco Corp   the country  39 s largest food se...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content    label\n",
       "count                                                7600     7600\n",
       "unique                                               7594        4\n",
       "top     Sysco Corp   the country  39 s largest food se...  SciTech\n",
       "freq                                                    2     1900"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7594</td>\n",
       "      <td>7594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>India posted 199/7 and trail Australia by 275 ...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content  label\n",
       "count                                                7594   7594\n",
       "unique                                               7594      4\n",
       "top     India posted 199/7 and trail Australia by 275 ...  World\n",
       "freq                                                    1   1900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(inplace = True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE com   TORONTO  Canada    A second team o...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP   A company founded by a chemistry research...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP   It s barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP   Southern California s smog fighting agenc...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     label\n",
       "0  Unions representing workers at Turner   Newall...  Business\n",
       "1  SPACE com   TORONTO  Canada    A second team o...   SciTech\n",
       "2  AP   A company founded by a chemistry research...   SciTech\n",
       "3  AP   It s barely dawn when Mike Fitzpatrick st...   SciTech\n",
       "4  AP   Southern California s smog fighting agenc...   SciTech"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverting labels from string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_label = list(set(label))\n",
    "data['labels'] = data['label'].apply(set_label.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>Business</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE com   TORONTO  Canada    A second team o...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP   A company founded by a chemistry research...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP   It s barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP   Southern California s smog fighting agenc...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     label  labels\n",
       "0  Unions representing workers at Turner   Newall...  Business       3\n",
       "1  SPACE com   TORONTO  Canada    A second team o...   SciTech       2\n",
       "2  AP   A company founded by a chemistry research...   SciTech       2\n",
       "3  AP   It s barely dawn when Mike Fitzpatrick st...   SciTech       2\n",
       "4  AP   Southern California s smog fighting agenc...   SciTech       2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train, content_test, labels_train, labels_test = train_test_split(data.content, data.labels, test_size=0.30, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(max_features = 15000)\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised_train_content = vectorizer.fit_transform(content_train)\n",
    "vectorised_test_content = vectorizer.transform(content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2279, 17101)\n",
      "(5315, 17101)\n"
     ]
    }
   ],
   "source": [
    "print(vectorised_test_content.shape)\n",
    "print(vectorised_train_content.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forming matrix for each documunts it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5315, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = to_categorical(labels_train)\n",
    "test_labels = to_categorical(labels_test)\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total unique words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17101"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len = len(vectorised_train_content.toarray()[1])\n",
    "input_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer : \n",
    "<ul>\n",
    "    <li>shape : 17101</li>\n",
    "</ul>\n",
    "No. of layers hidden layer: 2\n",
    "<ul>\n",
    "    <li>Length of each the layer : 50</li>\n",
    "    <li>Activation function : relu</li>\n",
    "</ul>\n",
    "Output layer : \n",
    "<ul>\n",
    "    <li>shape : 4</li>\n",
    "    <li>Activation function : sigmoid</li>\n",
    "</ul>\n",
    "Optimizer : RMSprop\n",
    "<br>Loss_function : binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_1():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(50, activation='relu', input_shape=(input_len,)))\n",
    "    model.add(layers.Dense(50, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics = [\"acc\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer : \n",
    "<ul>\n",
    "    <li>shape : 17101</li>\n",
    "</ul>\n",
    "No. of layers hidden layer: 1\n",
    "<ul>\n",
    "    <li>Length of the layer : 40</li>\n",
    "    <li>Activation function : relu</li>\n",
    "</ul>\n",
    "Output layer : \n",
    "<ul>\n",
    "    <li>shape : 4</li>\n",
    "    <li>Activation function : softmax</li>\n",
    "</ul>\n",
    "Optimizer : RMSprop\n",
    "<br>Loss_function : binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_2():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(40, activation='relu', input_shape=(input_len,)))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics = [\"acc\"] )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer : \n",
    "<ul>\n",
    "    <li>shape : 17101</li>\n",
    "</ul>\n",
    "No. of layers hidden layer: 2\n",
    "<ul>\n",
    "    <li>Length of the layer : 50</li>\n",
    "    <li>Activation function : relu</li>\n",
    "</ul>\n",
    "Output layer : \n",
    "<ul>\n",
    "    <li>shape : 4</li>\n",
    "    <li>Activation function : softmax</li>\n",
    "</ul>\n",
    "Optimizer : RMSprop\n",
    "<br>Loss_function : binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_3():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(50, activation='relu', input_shape=(input_len,)))\n",
    "    model.add(layers.Dense(50, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.mean_squared_error, metrics = [\"acc\"] )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer : \n",
    "<ul>\n",
    "    <li>shape : 17101</li>\n",
    "</ul>\n",
    "No. of layers hidden layer: 2\n",
    "<ul>\n",
    "    <li>Length of the layer : 50</li>\n",
    "    <li>Activation function : exponential</li>\n",
    "</ul>\n",
    "Output layer : \n",
    "<ul>\n",
    "    <li>shape : 4</li>\n",
    "    <li>Activation function : softmax</li>\n",
    "</ul>\n",
    "Optimizer : Adagrad\n",
    "<br>Loss_function : binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_4():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(50, activation='exponential', input_shape=(input_len,)))\n",
    "    model.add(layers.Dense(50, activation='exponential'))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adagrad(lr=0.01), loss=losses.mean_squared_error, metrics = [\"acc\"] )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer : \n",
    "<ul>\n",
    "    <li>shape : 17101</li>\n",
    "</ul>\n",
    "No. of layers hidden layer: 1\n",
    "<ul>\n",
    "    <li>Length of the layer : 50</li>\n",
    "    <li>Activation function : selu</li>\n",
    "</ul>\n",
    "Output layer : \n",
    "<ul>\n",
    "    <li>shape : 4</li>\n",
    "    <li>Activation function : sigmoid</li>\n",
    "</ul>\n",
    "Optimizer : Adagrad\n",
    "<br>Loss_function : binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_5():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(50, activation='selu', input_shape=(input_len,)))\n",
    "    model.add(layers.Dense(50, activation='selu'))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adagrad(lr=0.001), loss=losses.binary_crossentropy, metrics = [\"acc\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Network_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92904135, 0.93139098, 0.92763158, 0.93656014, 0.94172932,\n",
       "       0.92137477, 0.93926554, 0.93220339, 0.92372882, 0.92890773])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "cv_model_1 = KerasClassifier(build_fn = create_network_1, epochs=10, batch_size=50, verbose=0)\n",
    "cv_score_1 = cross_val_score(cv_model_1, vectorised_train_content, train_labels, cv=10 )\n",
    "cv_score_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Network_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93280076, 0.93984963, 0.93656015, 0.94078947, 0.94407895,\n",
       "       0.93314501, 0.93973634, 0.93926554, 0.93079097, 0.93455744])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "cv_model_2 = KerasClassifier(build_fn = create_network_2, epochs=10, batch_size=50, verbose=0)\n",
    "cv_score_2 = cross_val_score(cv_model_2, vectorised_train_content, train_labels, cv=10 )\n",
    "cv_score_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Network_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85150376, 0.85150376, 0.84586467, 0.86654135, 0.87218045,\n",
       "       0.84934087, 0.86817326, 0.85310735, 0.83804143, 0.85310735])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "cv_model_3 = KerasClassifier(build_fn = create_network_3, epochs=8, batch_size=100, verbose=0)\n",
    "cv_score_3 = cross_val_score(cv_model_3, vectorised_train_content, train_labels, cv=10 )\n",
    "cv_score_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Network_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84398495, 0.62406015, 0.85338346, 0.86090225, 0.88157894,\n",
       "       0.83427495, 0.87193973, 0.70433145, 0.83427493, 0.84180791])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(15)\n",
    "cv_model_4 = KerasClassifier(build_fn = create_network_4, epochs=10, batch_size=50, verbose=0)\n",
    "cv_score_4 = cross_val_score(cv_model_4, vectorised_train_content, train_labels, cv=10 )\n",
    "cv_score_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Network_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80827068, 0.83458647, 0.81860902, 0.80451129, 0.80733083,\n",
       "       0.80838042, 0.81450093, 0.80414313, 0.80838042, 0.82344632])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "cv_model_5 = KerasClassifier(build_fn = create_network_5, epochs=8, batch_size=50, verbose=0)\n",
    "cv_score_5 = cross_val_score(cv_model_5, vectorised_train_content, train_labels, cv=10 )\n",
    "cv_score_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Network_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5315/5315 [==============================] - 4s 659us/step - loss: 0.5481 - acc: 0.7476\n",
      "Epoch 2/10\n",
      "5315/5315 [==============================] - 3s 556us/step - loss: 0.3315 - acc: 0.8617\n",
      "Epoch 3/10\n",
      "5315/5315 [==============================] - 3s 547us/step - loss: 0.1831 - acc: 0.9416\n",
      "Epoch 4/10\n",
      "5315/5315 [==============================] - 3s 545us/step - loss: 0.1162 - acc: 0.9617\n",
      "Epoch 5/10\n",
      "5315/5315 [==============================] - 3s 549us/step - loss: 0.0823 - acc: 0.9729\n",
      "Epoch 6/10\n",
      "5315/5315 [==============================] - 3s 550us/step - loss: 0.0593 - acc: 0.9802\n",
      "Epoch 7/10\n",
      "5315/5315 [==============================] - 3s 559us/step - loss: 0.0427 - acc: 0.9865\n",
      "Epoch 8/10\n",
      "5315/5315 [==============================] - 3s 556us/step - loss: 0.0316 - acc: 0.9904\n",
      "Epoch 9/10\n",
      "5315/5315 [==============================] - 3s 564us/step - loss: 0.0225 - acc: 0.9936\n",
      "Epoch 10/10\n",
      "5315/5315 [==============================] - 3s 555us/step - loss: 0.0165 - acc: 0.9956\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "model_1 = create_network_1()\n",
    "neural_network_1 = model_1.fit(vectorised_train_content,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5315/5315 [==============================] - 3s 510us/step - loss: 0.5107 - acc: 0.7517\n",
      "Epoch 2/10\n",
      "5315/5315 [==============================] - 2s 438us/step - loss: 0.3649 - acc: 0.8320\n",
      "Epoch 3/10\n",
      "5315/5315 [==============================] - 2s 440us/step - loss: 0.2386 - acc: 0.9264\n",
      "Epoch 4/10\n",
      "5315/5315 [==============================] - 2s 467us/step - loss: 0.1625 - acc: 0.9543\n",
      "Epoch 5/10\n",
      "5315/5315 [==============================] - 2s 448us/step - loss: 0.1206 - acc: 0.9654\n",
      "Epoch 6/10\n",
      "5315/5315 [==============================] - 2s 438us/step - loss: 0.0946 - acc: 0.9733\n",
      "Epoch 7/10\n",
      "5315/5315 [==============================] - 2s 464us/step - loss: 0.0762 - acc: 0.9779 1s\n",
      "Epoch 8/10\n",
      "5315/5315 [==============================] - 2s 438us/step - loss: 0.0630 - acc: 0.9817\n",
      "Epoch 9/10\n",
      "5315/5315 [==============================] - 2s 444us/step - loss: 0.0520 - acc: 0.9849\n",
      "Epoch 10/10\n",
      "5315/5315 [==============================] - 2s 448us/step - loss: 0.0434 - acc: 0.9875\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "model_2 = create_network_2()\n",
    "neural_network_2 = model_2.fit(vectorised_train_content,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5315/5315 [==============================] - 3s 472us/step - loss: 0.1639 - acc: 0.6532\n",
      "Epoch 2/8\n",
      "5315/5315 [==============================] - 2s 406us/step - loss: 0.0847 - acc: 0.8875\n",
      "Epoch 3/8\n",
      "5315/5315 [==============================] - 2s 368us/step - loss: 0.0391 - acc: 0.9315\n",
      "Epoch 4/8\n",
      "5315/5315 [==============================] - 2s 365us/step - loss: 0.0218 - acc: 0.9580\n",
      "Epoch 5/8\n",
      "5315/5315 [==============================] - 2s 367us/step - loss: 0.0130 - acc: 0.9746\n",
      "Epoch 6/8\n",
      "5315/5315 [==============================] - 2s 366us/step - loss: 0.0077 - acc: 0.9851\n",
      "Epoch 7/8\n",
      "5315/5315 [==============================] - 2s 368us/step - loss: 0.0047 - acc: 0.9913\n",
      "Epoch 8/8\n",
      "5315/5315 [==============================] - 2s 378us/step - loss: 0.0032 - acc: 0.9936\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "model_3 = create_network_3()\n",
    "neural_network_3 = model_3.fit(vectorised_train_content,\n",
    "                    train_labels,\n",
    "                    epochs=8,\n",
    "                    batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5315/5315 [==============================] - 3s 587us/step - loss: 0.1513 - acc: 0.6239\n",
      "Epoch 2/10\n",
      "5315/5315 [==============================] - 3s 494us/step - loss: 0.0346 - acc: 0.9204\n",
      "Epoch 3/10\n",
      "5315/5315 [==============================] - 3s 505us/step - loss: 0.0142 - acc: 0.9678\n",
      "Epoch 4/10\n",
      "5315/5315 [==============================] - 3s 506us/step - loss: 0.0068 - acc: 0.9859\n",
      "Epoch 5/10\n",
      "5315/5315 [==============================] - 3s 509us/step - loss: 0.0043 - acc: 0.9917\n",
      "Epoch 6/10\n",
      "5315/5315 [==============================] - 3s 505us/step - loss: 0.0030 - acc: 0.9944\n",
      "Epoch 7/10\n",
      "5315/5315 [==============================] - 3s 518us/step - loss: 0.0025 - acc: 0.9953\n",
      "Epoch 8/10\n",
      "5315/5315 [==============================] - 3s 517us/step - loss: 0.0024 - acc: 0.9953\n",
      "Epoch 9/10\n",
      "5315/5315 [==============================] - 3s 523us/step - loss: 0.0021 - acc: 0.9959\n",
      "Epoch 10/10\n",
      "5315/5315 [==============================] - 3s 522us/step - loss: 0.0019 - acc: 0.9962\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "model_4 = create_network_4()\n",
    "neural_network_4 = model_4.fit(vectorised_train_content,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5315/5315 [==============================] - 3s 596us/step - loss: 0.5987 - acc: 0.7465\n",
      "Epoch 2/8\n",
      "5315/5315 [==============================] - 3s 510us/step - loss: 0.5268 - acc: 0.7500\n",
      "Epoch 3/8\n",
      "5315/5315 [==============================] - 3s 501us/step - loss: 0.4926 - acc: 0.7500\n",
      "Epoch 4/8\n",
      "5315/5315 [==============================] - 3s 562us/step - loss: 0.4631 - acc: 0.7509\n",
      "Epoch 5/8\n",
      "5315/5315 [==============================] - 3s 578us/step - loss: 0.4359 - acc: 0.7603\n",
      "Epoch 6/8\n",
      "5315/5315 [==============================] - 4s 696us/step - loss: 0.4109 - acc: 0.7844\n",
      "Epoch 7/8\n",
      "5315/5315 [==============================] - 3s 655us/step - loss: 0.3879 - acc: 0.8119\n",
      "Epoch 8/8\n",
      "5315/5315 [==============================] - 3s 544us/step - loss: 0.3669 - acc: 0.8395\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "model_5 = create_network_5()\n",
    "neural_network_5 = model_5.fit(vectorised_train_content,\n",
    "                    train_labels,\n",
    "                    epochs=8,\n",
    "                    batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279/2279 [==============================] - 1s 350us/step\n"
     ]
    }
   ],
   "source": [
    "loss_model_1, acc_model_1 = model_1.evaluate(vectorised_test_content, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = model_1.predict(vectorised_test_content)\n",
    "\n",
    "pred_label_1 = []\n",
    "for array in result_1:\n",
    "    pred_label_1.append(np.argmax(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraacy of model 1 :  0.9305616498464239\n",
      "Precision of model 1 :  0.9313893653516295\n",
      "F1 score of model 1  :  0.9354005167958657\n"
     ]
    }
   ],
   "source": [
    "prec_model_1 = precision_score(labels_test,pred_label_1, average=None)[0]\n",
    "f1_model_1 = f1_score(list(labels_test), pred_label_1, average = None)[0]\n",
    "\n",
    "print(\"Accuraacy of model 1 : \", acc_model_1)\n",
    "print(\"Precision of model 1 : \", prec_model_1)\n",
    "print(\"F1 score of model 1  : \", f1_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279/2279 [==============================] - 1s 290us/step\n"
     ]
    }
   ],
   "source": [
    "loss_model_2, acc_model_2 = model_2.evaluate(vectorised_test_content, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = model_2.predict(vectorised_test_content)\n",
    "\n",
    "pred_label_2 = []\n",
    "for array in result_2:\n",
    "    pred_label_2.append(np.argmax(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraacy of model 2 :  0.9393374287491045\n",
      "Precision of model 2 :  0.9483648881239243\n",
      "F1 score of model 2  :  0.9508196721311475\n"
     ]
    }
   ],
   "source": [
    "prec_model_2 = precision_score(labels_test,pred_label_2, average=None)[0]\n",
    "f1_model_2 = f1_score(list(labels_test), pred_label_2, average = None)[0]\n",
    "\n",
    "print(\"Accuraacy of model 2 : \", acc_model_2)\n",
    "print(\"Precision of model 2 : \", prec_model_2)\n",
    "print(\"F1 score of model 2  : \", f1_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279/2279 [==============================] - 1s 308us/step\n"
     ]
    }
   ],
   "source": [
    "loss_model_3, acc_model_3 = model_3.evaluate(vectorised_test_content, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = model_3.predict(vectorised_test_content)\n",
    "\n",
    "pred_label_3 = []\n",
    "for array in result_3:\n",
    "    pred_label_3.append(np.argmax(array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraacy of model 3 :  0.8617814831327796\n",
      "Precision of model 3 :  0.9366438356164384\n",
      "F1 score of model 3  :  0.9414802065404474\n"
     ]
    }
   ],
   "source": [
    "prec_model_3 = precision_score(labels_test,pred_label_3, average=None)[0]\n",
    "f1_model_3 = f1_score(list(labels_test), pred_label_3, average = None)[0]\n",
    "\n",
    "print(\"Accuraacy of model 3 : \", acc_model_3)\n",
    "print(\"Precision of model 3 : \", prec_model_3)\n",
    "print(\"F1 score of model 3  : \", f1_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279/2279 [==============================] - 1s 339us/step\n"
     ]
    }
   ],
   "source": [
    "loss_model_4, acc_model_4 = model_4.evaluate(vectorised_test_content, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictin labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_4 = model_4.predict(vectorised_test_content)\n",
    "\n",
    "pred_label_4 = []\n",
    "for array in result_4:\n",
    "    pred_label_4.append(np.argmax(array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraacy of model 4 :  0.860026327362705\n",
      "Precision of model 4 :  0.9344827586206896\n",
      "F1 score of model 4  :  0.9360967184801381\n"
     ]
    }
   ],
   "source": [
    "prec_model_4 = precision_score(labels_test,pred_label_4, average=None)[0]\n",
    "f1_model_4 = f1_score(list(labels_test), pred_label_4, average = None)[0]\n",
    "\n",
    "print(\"Accuraacy of model 4 : \", acc_model_4)\n",
    "print(\"Precision of model 4 : \", prec_model_4)\n",
    "print(\"F1 score of model 4  : \", f1_model_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279/2279 [==============================] - 1s 322us/step\n"
     ]
    }
   ],
   "source": [
    "loss_model_5, acc_model_5 = model_5.evaluate(vectorised_test_content, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictin labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_5 = model_5.predict(vectorised_test_content)\n",
    "\n",
    "pred_label_5 = []\n",
    "for array in result_5:\n",
    "    pred_label_5.append(np.argmax(array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraacy of model 4 :  0.8313953488633632\n",
      "Precision of model 4 :  0.9095394736842105\n",
      "F1 score of model 4  :  0.9325463743676222\n"
     ]
    }
   ],
   "source": [
    "prec_model_5 = precision_score(labels_test,pred_label_5, average=None)[0]\n",
    "f1_model_5 = f1_score(list(labels_test), pred_label_5, average = None)[0]\n",
    "\n",
    "print(\"Accuraacy of model 4 : \", acc_model_5)\n",
    "print(\"Precision of model 4 : \", prec_model_5)\n",
    "print(\"F1 score of model 4  : \", f1_model_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output_of_topic_classifier.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t\\t\\t Details of the dataset\", file = f)\n",
    "print(\"Number of data used for training ::\", len(content_train), file = f)\n",
    "print(\"Number of data used for testing ::\", len(content_test), file = f)\n",
    "print(\"Number of different model used :: 5\", file = f)\n",
    "print(\"\\n\", file = f)\n",
    "\n",
    "for i in range(1,6):\n",
    "\n",
    "    ######################################################################################################################\n",
    "    neural_network = vars()[\"neural_network_\" + str(i)]\n",
    "    cv_score = vars()[\"cv_score_\" + str(i)]\n",
    "    acc_model = vars()[\"acc_model_\" + str(i)]\n",
    "    prec_model = vars()[\"prec_model_\" + str(i)]\n",
    "    f1_model = vars()[\"f1_model_\" + str(i)]\n",
    "    model = vars()[\"model_\" + str(i)]\n",
    "    model_summary = model.get_config()[\"layers\"]\n",
    "    ######################################################################################################################\n",
    "\n",
    "    ######################################################################################################################\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t MODEL\",i, file = f)\n",
    "    ######################################################################################################################\n",
    "\n",
    "    ######################################################################################################################\n",
    "    print(\"\\t Structutre of the Network {} \" .format(i), file = f)\n",
    "    print(\"\\n\", file = f)\n",
    "    no_of_layers = len(model_summary)\n",
    "\n",
    "    print(\"Number of layers in the model ::\", no_of_layers, file = f)\n",
    "    for i in range(0, no_of_layers - 1):\n",
    "        print(\"No. of nodes in layer {} ::\" .format(i+1), model_summary[i][\"config\"][\"units\"], file = f)\n",
    "        print(\"Activation function of layer {} ::\" .format(i+1),model_summary[i][\"config\"][\"activation\"], file = f)    \n",
    "    print(\"No. of nodes in output layer ::\", model_summary[no_of_layers - 1][\"config\"][\"units\"], file = f)\n",
    "    print(\"Activation function of output layer ::\", model_summary[no_of_layers - 1][\"config\"][\"activation\"], file = f)\n",
    "    ######################################################################################################################\n",
    "\n",
    "    ######################################################################################################################\n",
    "    print(\"\\n\", file = f)\n",
    "    print(\"\\t Details of the model no {}\" .format(i), file = f)\n",
    "    print(\"\\n\", file = f)\n",
    "    print(\"batch_size ::\", neural_network.params['batch_size'], file = f)\n",
    "    print(\"epochs ::\", neural_network.params['epochs'], file = f)\n",
    "    print(\"Cost function used ::\", str(model.loss_functions[0]).split(\".\")[0].split(\" \")[1], file = f)\n",
    "    print(\"Optimizer used ::\", str(model.optimizer).split(\".\")[2].split(\" \")[0], file = f)\n",
    "    ######################################################################################################################\n",
    "\n",
    "    ######################################################################################################################\n",
    "    print(\"\\n\", file = f)\n",
    "    print(\"\\t Evaluation \", file = f)\n",
    "    print(\"\\n\", file = f)\n",
    "    print(\"Ten fold cross validation of model {} is : \".format(i), list(map(lambda x: round(x * 100,2), cv_score)), file = f)\n",
    "    ######################################################################################################################\n",
    "\n",
    "    ######################################################################################################################\n",
    "    print(\"Accuracy is : \", acc_model, file = f)\n",
    "    print(\"Precision is : \", prec_model, file = f)\n",
    "    print(\"F1 score is : \", f1_model, file = f)\n",
    "    print(\"\\n\", file = f)\n",
    "    ######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
